{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick WL galaxy cluster mass determination example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookup table loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#plots\n",
    "from matplotlib import pyplot as plt\n",
    "from getdist import plots, MCSamples\n",
    "\n",
    "#astrophysics\n",
    "#import galsim\n",
    "import xlensing\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "#saving\n",
    "from astropy.table import Table\n",
    "import pickle\n",
    "\n",
    "#MCMC\n",
    "import emcee\n",
    "\n",
    "#utilities\n",
    "#import os\n",
    "import time\n",
    "import warnings\n",
    "import tqdm\n",
    "\n",
    "#let's use multiprocessing\n",
    "from multiprocessing import Pool, freeze_support, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def truncnorm(low, high, loc, sigma, size): #I hate scipy truncnorm\n",
    "    x = []\n",
    "    while len(x)<size:\n",
    "        xi = np.random.normal(loc,sigma)\n",
    "        if xi < high and xi > low:\n",
    "            x.append(xi)\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random distribution of non-cluster galaxies (not accurate with LSS):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def gen_gal(Ngals=1e5,Zcluster=.3,rmax=12., gmax=.6,gspread=.1):\n",
    "    Ngals = int(Ngals)\n",
    "    Zgals = np.random.uniform(0,1,size=int(Ngals))\n",
    "\n",
    "    angular_distance = xlensing.cosmo.cosmology.angular_diameter_distance(Zcluster).value #Mpc\n",
    "    r_galaxies = np.random.uniform(0,rmax,size=Ngals) #Mpc\n",
    "    theta_galaxies = np.random.uniform(0,2*np.pi, size=Ngals)\n",
    "\n",
    "    RAgals = r_galaxies*np.cos(theta_galaxies)/angular_distance\n",
    "    DECgals= r_galaxies*np.sin(theta_galaxies)/angular_distance\n",
    "\n",
    "    #intrinsic ellipticity of the galaxies\n",
    "    Ebarr = 0.1\n",
    "    E1gals = truncnorm(-gmax,gmax,0,Ebarr,Ngals)\n",
    "    E2gals = truncnorm(-gmax,gmax,0,Ebarr,Ngals)\n",
    "    return E1gals, E2gals, RAgals, DECgals, Zgals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shear background galaxies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply NFW-like shear\n",
    "def gNFW(x):\n",
    "    if x<1:\n",
    "        g = 8*np.arctanh(np.sqrt((1-x)/(1+x)))/(x**2*np.sqrt(1-x**2)) + 4/(x**2)*np.log(x/2) - 2/(x**2-1) + 4*np.arctanh(np.sqrt((1-x)/(1+x)))/( (x**2-1)*np.sqrt(1-x**2) )\n",
    "    if x>1:\n",
    "        g = 8*np.arctan (np.sqrt((x-1)/(1+x)))/(x**2*np.sqrt(x**2-1)) + 4/(x**2)*np.log(x/2) - 2/(x**2-1) + 4*np.arctan (np.sqrt((x-1)/(1+x)))/( (x**2-1)*np.sqrt(x**2-1) )\n",
    "    if x ==1:\n",
    "        g = 10/3 +4*np.log(1/2)\n",
    "    return g    \n",
    "\n",
    "def NFW_shear(cluster, galaxies):\n",
    "    \n",
    "    #unpack data\n",
    "    Mvir, conc, centre_RA, centre_DEC, z_lens = cluster\n",
    "    gal_RA, gal_DEC, z_galaxy, e1, e2  = galaxies\n",
    "    \n",
    "    #find physical radius in the plane of the sky\n",
    "    sep, theta = xlensing.data.equatorial_to_polar(gal_RA, gal_DEC, centre_RA, centre_DEC)\n",
    "    ang_dist = xlensing.cosmo.cosmology.angular_diameter_distance(z_lens).value\n",
    "    r = sep*ang_dist\n",
    "    theta= theta +np.pi/2 #the polar angle is 90ยบ different from the tangent (I took 6 years to figure this out)\n",
    "    \n",
    "    #get complex shape\n",
    "    intr_shape = e1 + e2*1.j\n",
    "    \n",
    "    Ngals = len(intr_shape)\n",
    "    \n",
    "    #define scale radius of NFW\n",
    "    rs= xlensing.model.r_vir(z_lens,Mvir)/conc\n",
    "    \n",
    "    #NFW kappa factor\n",
    "    fact =  rs*xlensing.model.NFW_delta_c(conc)*xlensing.cosmo.rhoM(z_lens)\n",
    "    \n",
    "    #Rescaled radiii\n",
    "    x = r/rs\n",
    " \n",
    "    #critical surface density\n",
    "    sigcrit = xlensing.data.sigmacrit(z_lens,z_galaxy)\n",
    "\n",
    "    \n",
    "    #total shear caused by NFW profile (Wright, Brainerd, 99)\n",
    "    gammat = (fact/sigcrit)*np.array([gNFW(r) for r in x]) #add 1% error to measurements\n",
    "    \n",
    "    #complex gamma, reduced shear\n",
    "    gamma = gammat*np.exp(2.j*(theta))\n",
    "    \n",
    "    \n",
    "    #gt = gamma/(1+kappa)\n",
    "\n",
    "    \n",
    "    #final shape\n",
    "    epsilon = np.empty(Ngals,dtype='complex')\n",
    "    for i in range(Ngals):\n",
    "        if z_galaxy[i] > z_lens:\n",
    "            epsilon[i] =  intr_shape[i] + gamma[i]#\n",
    "        else:\n",
    "            epsilon[i] =  intr_shape[i]\n",
    "    return epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(cluster_backgrounds,bin_limits,Nboot=200):\n",
    "    \"\"\"\n",
    "    cluster_backgrounds = a list of ndarrays, each containing \n",
    "    cluster background galaxies for lensing. They should contain: \n",
    "    - (0) Sigma_crit: the critical density calculated from the cluster and galaxy redshifts\n",
    "    - (1) e_t: the tangential component of the shear\n",
    "    - (2) e_x: the cross component of the shear\n",
    "    - (3) W: the weight of the ellipticity measurement\n",
    "    - (4) R: the angular diameter radius in Mpc/h between the cluster centre and the background\n",
    "          galaxy position.\n",
    "    - (5) M: the estimation of multiplicative biases \n",
    "\n",
    "    \n",
    "    bin_limits = an array containing the bin lower and upper bounds\n",
    "    \n",
    "    Nboot = the number of resamplings desired\n",
    "    \n",
    "    TODO: invert resampling bin structure in 1-cluster stacks\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Total galaxies available per bin:\")\n",
    "    sources_radii = np.hstack([cluster_backgrounds[i][4] for i in range(len(cluster_backgrounds))])\n",
    "    print([len(sources_radii[(sources_radii > bini[0]) & (sources_radii < bini[1])]) for bini in bin_limits ] )\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    Nbins=len(bin_limits)\n",
    "    \n",
    "    ##for single cluster stacks\n",
    "    if len(cluster_backgrounds) == 1:\n",
    "        print(\"Single cluster:\")\n",
    "        background = cluster_backgrounds[0]\n",
    "        print(\"Separating galaxies per radial bin...\")\n",
    "        #separate all available galaxies into radial bins\n",
    "        radial_background = []\n",
    "\n",
    "        for bins in bin_limits:\n",
    "            bin_upper_cut  = background[:,background[4]<bins[1]]\n",
    "            bin_lower_cut  = bin_upper_cut[:,bin_upper_cut[4]>bins[0]]\n",
    "            radial_background.append(bin_lower_cut)\n",
    "            \n",
    "        Delta_Sigmas = np.empty((Nboot,Nbins)) #sigma_crit * et (E-mode signal)\n",
    "        Delta_Xigmas = np.empty((Nboot,Nbins)) #sigma_crit * ex (B-mode signal)\n",
    "       \n",
    "        #bootstrap\n",
    "        for sampleNo in range(Nboot):\n",
    "\n",
    "            for radius, radial_bin in enumerate(radial_background):\n",
    "                \n",
    "                sorted_galaxies = np.random.randint(0,len(radial_bin.T),len(radial_bin.T))\n",
    "                sorted_bin = np.array([radial_bin.T[i] for i in sorted_galaxies]).T\n",
    "                \n",
    "                Sigma = np.average(sorted_bin[0,:]*sorted_bin[1,:],weights= sorted_bin[3,:]/(sorted_bin[0,:]**2))\n",
    "                Xigma = np.average(sorted_bin[0,:]*sorted_bin[2,:],weights= sorted_bin[3,:]/(sorted_bin[0,:]**2))\n",
    "                \n",
    "                #average multiplicative bias correction\n",
    "                One_plus_K = np.average(sorted_bin[5,:]+1,weights= sorted_bin[3,:]/(sorted_bin[0,:]**2))\n",
    "\n",
    "                Delta_Sigmas[sampleNo,radius] = Sigma/One_plus_K\n",
    "                Delta_Xigmas[sampleNo,radius] = Xigma/One_plus_K\n",
    "                \n",
    "           \n",
    "        Delta_Sigmas = np.array(Delta_Sigmas)\n",
    "        Delta_Xigmas = np.array(Delta_Xigmas)\n",
    "        \n",
    "        #gather results\n",
    "        sigmas = np.mean(Delta_Sigmas,axis=0)\n",
    "        xigmas = np.mean(Delta_Xigmas,axis=0)\n",
    "        \n",
    "        sigmas_cov = np.cov(Delta_Sigmas.T)\n",
    "        xigmas_cov = np.cov(Delta_Xigmas.T)\n",
    "\n",
    "    else:\n",
    "        print(\"Stack of {} clusters.\".format(len(cluster_backgrounds)))\n",
    "        #sorts Nboot selections of the clusters all at once\n",
    "        resample=np.random.randint(0,len(cluster_backgrounds),(Nboot,len(cluster_backgrounds)))\n",
    "        \n",
    "        Delta_Sigmas = np.empty((Nbins,Nboot)) #E-mode signal (tang. shear)\n",
    "        Delta_Xigmas = np.empty((Nbins,Nboot)) #B-mode signal (cross shear)\n",
    "        \n",
    "        #bootstrap\n",
    "        for sampleNo in range(len(resample)):\n",
    "            stake = np.hstack([cluster_backgrounds[i] for i in resample[sampleNo]])\n",
    "\n",
    "            for radius in range(Nbins):\n",
    "                #populate radial bins\n",
    "                bin_upper_cut = stake[:,stake[4,:]<bin_limits[radius,1]]\n",
    "                bin_cut = bin_upper_cut[:,bin_upper_cut[4,:]>bin_limits[radius,0]]\n",
    "                \n",
    "                #sigma = average sigma_crit * shear * weight=(W/sigma_crit^2)\n",
    "                Sigma = np.average(bin_cut[0,:]*bin_cut[1,:],weights= bin_cut[3,:]/(bin_cut[0,:]**2))\n",
    "                Xigma = np.average(bin_cut[0,:]*bin_cut[2,:],weights= bin_cut[3,:]/(bin_cut[0,:]**2))\n",
    "                \n",
    "                #average multiplicative bias correction\n",
    "                One_plus_K = np.average(bin_cut[5,:]+1,weights= bin_cut[3,:]/(bin_cut[0,:]**2))\n",
    "\n",
    "                Delta_Sigmas[radius,sampleNo] = Sigma/One_plus_K\n",
    "                Delta_Xigmas[radius,sampleNo] = Xigma/One_plus_K\n",
    "\n",
    "            del stake\n",
    "            \n",
    "        #gather results\n",
    "        sigmas = np.mean(Delta_Sigmas,axis=0)\n",
    "        xigmas = np.mean(Delta_Xigmas,axis=0)\n",
    "        \n",
    "        sigmas_cov = np.cov(Delta_Sigmas.T)\n",
    "        xigmas_cov = np.cov(Delta_Xigmas.T)\n",
    "    \n",
    "\n",
    "    return sigmas, sigmas_cov, xigmas, xigmas_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_gal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-96ea643ae5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDECluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;31m#radians\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mE1gals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE2gals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRAgals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDECgals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZgals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_gal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNgals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNgals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZcluster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_gal' is not defined"
     ]
    }
   ],
   "source": [
    "Ngals = 1e5\n",
    "M200true = 2e14\n",
    "C200true = 3.5\n",
    "Zcluster = 0.3\n",
    "RAcluster = 0.0 #radians \n",
    "DECluster = 0.0 #radians\n",
    "\n",
    "E1gals, E2gals, RAgals, DECgals, Zgals = gen_gal(Ngals=Ngals,Zcluster=Zcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = NFW_shear( (M200true,C200true, 0, 0, .3),(RAgals, DECgals, Zgals, E1gals, E2gals) )\n",
    "e1gals = np.real(epsilon)\n",
    "e2gals = np.imag(epsilon)\n",
    "\n",
    "e1err = np.array([np.abs(np.random.normal(e/100,np.abs(e/20))) for e in e1gals])\n",
    "e2err = np.array([np.abs(np.random.normal(e/100,np.abs(e/20))) for e in e2gals])\n",
    "Wgals = (0.1**2 + e1err**2 +e2err**2)/(0.1**2 + e1err**2 +e2err**2) #w=1\n",
    "Mgals = -np.random.exponential(0.03,size=int(Ngals))*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_catalog = Table([RAgals,DECgals,Zgals,e1gals,e2gals, Wgals,Mgals],names=['RA','DEC','ZPHOT','E1','E2','WEIGHT','M'])\n",
    "#galaxy_catalog.write('mock_galaxy_catalog.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_catalog = Table([[RAcluster],[DECluster],[Zcluster]],names=['RA','DEC','Z'])\n",
    "#cluster_catalog.write('mock_cluster_catalog.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using xlensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having only one cluster, we will pretend there are many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources = Table.read('mock_galaxy_catalog.fits')\n",
    "clusters = Table([[0],[0],[0.3]],names=['RA','DEC', 'Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpack clusters and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_RA = np.array(galaxy_catalog['RA'])\n",
    "sr_DEC= np.array(galaxy_catalog['DEC'])\n",
    "sr_z  = np.array(galaxy_catalog['ZPHOT'])\n",
    "sr_E1 = np.array(galaxy_catalog['E1'])\n",
    "sr_E2 = np.array(galaxy_catalog['E2'])\n",
    "sr_W = np.array(galaxy_catalog['WEIGHT'])\n",
    "sr_M = np.array(galaxy_catalog['M'])\n",
    "\n",
    "clusters['INDEX'] = np.array(range(len(clusters)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_RA=np.array(clusters['RA'])\n",
    "cl_DEC= np.array(clusters['DEC'])\n",
    "cl_z= np.array(clusters['Z'])\n",
    "cl = np.array([cl_RA,cl_DEC,cl_z]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first step, the code calculates e_t and sigma_crit for all clusters and all available galaxies in our survey. Galaxies are selected as background  for a cluster within a radius from the centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use whatever, but your machine core count is usually a good choice (although maybe not the best)\n",
    "pool = Pool(cpu_count()) \n",
    "\n",
    "#We get a partial function with a constant galaxy catalogue to iterate with clusters.\n",
    "survey_lensing = partial(xlensing.data.cluster_lensing,sources=(sr_RA, sr_DEC, sr_z, sr_E1, sr_E2, sr_W,sr_M),radius=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of clusters to get lensing data\n",
    "clz = zip(cl_RA,cl_DEC,cl_z)\n",
    "clzlist = [x for x in clz]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step needs multiprocessing because for a real survey, ~1e3 clusters and more than 1e6 galaxies take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pool.map(survey_lensing, clzlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the resulting list\n",
    "#pickle_out = open(\"mock_results.pickle\",\"wb\")\n",
    "#pickle.dump(results, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate the product of our first step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a dictionary, containing entries for:\n",
    "\n",
    "- Critical density: the critical surface density at the background galaxy redshift, for each galaxy\n",
    "- Tangential shear: the tangential (relative to the cluster centre) component of the shear\n",
    "- Cross shear: the cross component of the shear\n",
    "- Radial distance: the separation between the background galaxy and the cluster centre in physical units (Mpc, angular diameter distance in the plane of the cluster)\n",
    "- Polar angle: the azimuthal angle of the galaxy position relative to the cluster RA great circle\n",
    "- Weights: lensifit weights\n",
    "- Mult. Bias: the estimated multiplicative bias for the galaxy shape measurement\n",
    "- Count: the number of galaxies considered \"background\"\n",
    " \n",
    "Except for count, which is a number, all other dictionary keys should contain arrays of \"Count\" values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a radial bin structure for modelling the mass distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = np.logspace(-0.8,0.8,8)\n",
    "N = len(radii)\n",
    "bins_lims = np.logspace(np.log10(radii[0])+(np.log10(radii[0])-np.log10(radii[1]))/2,\n",
    "                        np.log10(radii[N-1])-(np.log10(radii[0])-np.log10(radii[1]))/2,N+1)\n",
    "bins_lims = np.array([[bins_lims[i],bins_lims[i+1]] for i in range(N)])\n",
    "bins_lims #in Mpc/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a \"stick\" is a list containing a set of \"stacks\", a \"stack\" is table containing a set of clusters. Since we have only 1 cluster, we will make a stick of 1 stack containing just a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stick = [clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nboot=200\n",
    "stick_results = []\n",
    "for stake in stick:\n",
    "    t = time.time()\n",
    "    clusterbkgs = []\n",
    "    for index in stake['INDEX']:\n",
    "        Sigma_crit = np.array(results[index]['Critical Density'])\n",
    "        e_t = np.array(results[index]['Tangential Shear'])\n",
    "        e_x = np.array(results[index]['Cross Shear'])\n",
    "        W = np.array(results[index]['Weights'])\n",
    "        M = np.array(results[index]['Mult. Bias'])\n",
    "        R = np.array(results[index]['Radial Distance'])\n",
    "        clusterbkgs.append(np.array([Sigma_crit, e_t, e_x, W, R,M]))\n",
    "    print(len(clusterbkgs))\n",
    "    sigmas, sigmas_cov, xigmas, xigmas_cov = stack(clusterbkgs,bins_lims,Nboot)\n",
    "    stick_results.append( ( sigmas, sigmas_cov, xigmas, xigmas_cov) )\n",
    "    print(\"Done in \" + str(time.time()-t) + \" seconds.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii_model = np.logspace(np.log10(2e-2),2,500)\n",
    "rs= xlensing.model.r_vir(Zcluster,M200true)/C200true\n",
    "x = radii_model/rs\n",
    "fact =  rs*xlensing.model.NFW_delta_c(C200true)*xlensing.cosmo.rhoM(Zcluster)/1e12\n",
    "trueg = fact*np.array([gNFW(r) for r in x])\n",
    "plt.figure(figsize=(7,9))\n",
    "for stickresult in stick_results:\n",
    "    plt.scatter(radii,np.array(stickresult[0]))\n",
    "    plt.errorbar(radii,np.array(stickresult[0]),yerr=np.sqrt(np.diag(stickresult[1])),fmt='.')\n",
    "\n",
    "plt.plot(radii_model,trueg,label='True')\n",
    "plt.legend()\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('A nice, beautiful cluster')\n",
    "plt.savefig('mock.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simplified version that takes only mass and concentration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFWsimple(theta,Z,radii):\n",
    "    logM200, C200  = theta\n",
    "    M200 = np.power(10,logM200)\n",
    "    result = xlensing.model.NFW_shear(M200, C200, Z, 1.0, 0.001, 1e10,radii)['NFW Signal'] #returns only the main shear signal - all other signals (incl cross signal) available see docstring\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M200lo, M200hi = 13, 15\n",
    "C200lo, C200hi = 0, 10\n",
    "\n",
    "priorM200 = xlensing.fitting.ln_flat_prior_maker(M200lo, M200hi,0)\n",
    "priorC200 = xlensing.fitting.ln_flat_prior_maker(C200lo, C200hi,1)\n",
    "#priorPCC = xlensing.fitting.ln_gaussian_prior_maker(0.75, 0.07,2) ##Zhang et al. 2019\n",
    "prior = lambda theta : priorM200(theta) + priorC200(theta)# + priorPCC(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalkers, steps = 2, 256, 256\n",
    "samplestick = []\n",
    "#for each stack, run MCMC\n",
    "burnin=round(steps/4.)\n",
    "for stickresult in stick_results:\n",
    "\n",
    "    mean_z = Zcluster\n",
    "\n",
    "    #build data likelihood\n",
    "    model = lambda theta: NFWsimple(theta,mean_z,radii)\n",
    "    likelihood = xlensing.fitting.ln_gaussian_likelihood_maker((stickresult[0],stickresult[1]),model)\n",
    "    posterior = lambda theta : likelihood(theta) +prior(theta)\n",
    "\n",
    "    #initialise walkers\n",
    "    pos = []\n",
    "    for i in range(nwalkers):\n",
    "        M200 = np.random.uniform(M200lo,M200hi)\n",
    "        C200 = np.random.uniform(C200lo,C200hi)\n",
    "        #PCC  = np.random.uniform(PCClo,PCChi)\n",
    "        pos.append(np.array([M200,C200]))\n",
    "\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, posterior)\n",
    "    print(\"Running MCMC...\")\n",
    "    t = time.time()\n",
    "    sampler.run_mcmc(pos, steps, rstate0=np.random.get_state())\n",
    "    print(\"Done in \" + str(time.time()-t) + \" seconds.\")\n",
    "    samples = sampler.chain[:, burnin:, :].reshape((-1, ndim))\n",
    "    samplestick.append(samples)\n",
    "\n",
    "timestamp = time.time()\n",
    "#don't mind the warnings for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for samples in samplestick:\n",
    "    mvir_tru,conc_tru= map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(samples, [16, 50, 84],axis=0)))\n",
    "    print(\"Mvir: {:.2e}\".format(10**mvir_tru[0]) + \" p {:.2e}\".format(mvir_tru[1]) + \" m {:.2e}\".format(mvir_tru[2]))\n",
    "    print(\"Conc: {:.2f}\".format(conc_tru[0]) + \" p {:.2f}\".format(conc_tru[1]) + \" m {:.2f}\".format(conc_tru[2]))\n",
    "    #print(\"Pcc: {:.2f}\".format(pcc_tru[0]) + \" p {:.2f}\".format(pcc_tru[1]) + \" m {:.2f}\".format(pcc_tru[2]))\n",
    "\n",
    "labs =  [\"M_{200} [M_\\\\odot]\", \"c_{200}\"]#,\"p_{cc}\"]\n",
    "\n",
    "g = plots.getSubplotPlotter()\n",
    "sample = [MCSamples(samples=samples, names = labs,labels=labs) for samples in samplestick]\n",
    "g.triangle_plot(sample,filled=True)\n",
    "g.fig.savefig(f'posterior_test_{timestamp}.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "for stickresult in stick_results:\n",
    "    plt.scatter(radii,np.array(stickresult[0]))\n",
    "    plt.errorbar(radii,np.array(stickresult[0]),yerr=np.sqrt(np.diag(stickresult[1])),fmt='.')\n",
    "    \n",
    "    \n",
    "Model = xlensing.model.NFW_shear(10**mvir_tru[0],#M200\n",
    "                                  conc_tru[0],#C200\n",
    "                                  0.3,#Z\n",
    "                                  1,#pcc\n",
    "                                  0.01,#sigma_off\n",
    "                                  1e10,#M0\n",
    "                                  radii=radii_model)['NFW Signal'] \n",
    "plt.plot(radii_model,Model,label='Medians')\n",
    "Model = xlensing.model.NFW_shear(10**np.average(samples,axis=0)[0],#M200\n",
    "                                  np.average(samples,axis=0)[1],#C200\n",
    "                                  0.3,#Z\n",
    "                                  1,#pcc\n",
    "                                  0.01,#sigma_off\n",
    "                                  1e10,#M0\n",
    "                                  radii=radii_model)['NFW Signal'] \n",
    "\n",
    "\n",
    "\n",
    "plt.plot(radii_model,Model,label='Means')\n",
    "\n",
    "\n",
    "Model = xlensing.model.NFW_shear(M200true,#M200\n",
    "                                  C200true,#C200\n",
    "                                  0.35,#Z\n",
    "                                  1,#pcc\n",
    "                                  0.01,#sigma_off\n",
    "                                  1e10,#M0\n",
    "                                  radii=radii_model)['NFW Signal'] \n",
    "\n",
    "m_ratio = 10**mvir_tru[0]/M200true\n",
    "\n",
    "c_ratio = conc_tru[0]/C200true\n",
    "plt.plot(radii_model,Model,label='True')\n",
    "plt.legend(fontsize=20)\n",
    "#plt.ylim(1e1,3e3)\n",
    "#plt.xlim(.8e-1,2e1)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('$\\Delta \\Sigma$  [$h$ M$_{\\odot} \\cdot$ pc$^{-2}$] ',fontsize=20)\n",
    "plt.xlabel('R [$h^{-1}$ Mpc]',fontsize=20)\n",
    "plt.title(f'mass median / true mass= {m_ratio:.2f}, concentration median / true concentration = {c_ratio:.2f}',fontsize=20)\n",
    "plt.savefig(f'fit_example_medians_means_{timestamp}.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model = xlensing.model.NFW_shear(44.14e12,#M200\n",
    "                                  3.97,#C200\n",
    "                                  0.25,#Z\n",
    "                                  .625,#pcc\n",
    "                                  0.32,#sigma_off\n",
    "                                  .32e12,#M0\n",
    "                                  radii=radii_model) #returns a dict\n",
    "raios = Model['radii']\n",
    "Model_total = Model['Signal']\n",
    "Model_BCG = Model['BCG Signal']\n",
    "Model_NFW_centre = Model['NFW Signal']\n",
    "Model_NFW_miscc = Model['Miscentered Signal']\n",
    "Model_2ht = Model['Two-halo term']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,9))\n",
    "factor = 1e12\n",
    "plt.title('$M_{200}=44 \\\\times 10^{12} h^{-1}  \\mathrm{M}_\\odot$, $c_{200}=3.97$, $z$ = 0.25, \\n $p_{cc}=0.625$, $\\\\sigma_{off}=0.32 h^{-1} \\mathrm{Mpc}$, $M_{0} = .32\\\\times10^{12} h^{-1} \\mathrm{M}_\\odot$ ',fontsize=15)\n",
    "plt.xscale('log') \n",
    "plt.yscale('log')\n",
    "plt.plot(raios,Model_total,linewidth=3,color='magenta',label=\"$\\Delta \\Sigma $ Total\")\n",
    "plt.plot(raios,Model_BCG,linewidth=3,color='red',label=\"BCG point mass\")\n",
    "plt.plot(raios,Model_NFW_centre,linewidth=3,color='green',label=\"$\\Delta \\Sigma_{NFW} $\")\n",
    "plt.plot(raios,Model_NFW_miscc,linewidth=3,color='orange',label=\"$\\Delta \\Sigma_{NFW}^{off}$\")\n",
    "plt.plot(raios,Model_2ht,linewidth=3,color='blue',label=\"$\\Delta \\Sigma_{2h}$\")\n",
    "plt.xlim(2e-2,3.5e1)\n",
    "plt.ylim(1e-2,3.5e3)\n",
    "plt.legend(loc=1,fontsize=13)\n",
    "plt.ylabel('$\\\\Delta \\\\Sigma (R)$ $[h \\mathrm{M}_{\\odot} \\cdot \\mathrm{pc}^{-2}]$ ',fontsize=20)\n",
    "plt.xlabel('$R$ $[\\mathrm{Mpc}/h]$',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('modeltest.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 fittings of the same cluster with different backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create galaxies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M200true = 2e14\n",
    "C200true = 3.5\n",
    "Zcluster = 0.3\n",
    "RAcluster = 0.0 #radians \n",
    "DECluster = 0.0 #radians\n",
    "Ngals = 1e5\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "  \n",
    "  E1gals, E2gals, RAgals, DECgals, Zgals = gen_gal(Ngals=Ngals,Zcluster=Zcluster)\n",
    "\n",
    "\n",
    "  epsilon = NFW_shear( (M200true,C200true, 0, 0, .3),(RAgals, DECgals, Zgals, E1gals, E2gals) )\n",
    "  e1gals = np.real(epsilon)\n",
    "  e2gals = np.imag(epsilon)\n",
    "\n",
    "  e1err = np.array([np.abs(np.random.normal(e/100,np.abs(e/20))) for e in e1gals])\n",
    "  e2err = np.array([np.abs(np.random.normal(e/100,np.abs(e/20))) for e in e2gals])\n",
    "  Wgals = (0.1**2 + e1err**2 +e2err**2)/(0.1**2 + e1err**2 +e2err**2) #w=1\n",
    "  Mgals = -np.random.exponential(0.03,size=int(Ngals))*0\n",
    "  \n",
    "  galaxy_catalog = Table([RAgals,DECgals,Zgals,e1gals,e2gals, Wgals,Mgals],names=['RA','DEC','ZPHOT','E1','E2','WEIGHT','M'])\n",
    "\n",
    "\n",
    "  sr_RA = np.array(galaxy_catalog['RA'])\n",
    "  sr_DEC= np.array(galaxy_catalog['DEC'])\n",
    "  sr_z  = np.array(galaxy_catalog['ZPHOT'])\n",
    "  sr_E1 = np.array(galaxy_catalog['E1'])\n",
    "  sr_E2 = np.array(galaxy_catalog['E2'])\n",
    "  sr_W = np.array(galaxy_catalog['WEIGHT'])\n",
    "  sr_M = np.array(galaxy_catalog['M'])\n",
    "\n",
    "\n",
    "  clusters = Table([[RAcluster],[DECluster],[Zcluster]],names=['RA','DEC', 'Z'])\n",
    "  clusters['INDEX'] = np.array(range(len(clusters)))\n",
    "\n",
    "  pool = Pool(cpu_count()) \n",
    "\n",
    "\n",
    "  #We get a partial function with a constant galaxy catalogue to iterate with clusters.\n",
    "  \n",
    "  survey_lensing = partial(xlensing.data.cluster_lensing,sources=(sr_RA, \n",
    "                                                                  sr_DEC, \n",
    "                                                                  sr_z, \n",
    "                                                                  sr_E1, \n",
    "                                                                  sr_E2, \n",
    "                                                                  sr_W,\n",
    "                                                                  sr_M),radius=10.)\n",
    "\n",
    "  #Make a list of clusters to get lensing data\n",
    "  cl_RA=np.array(clusters['RA'])\n",
    "  cl_DEC= np.array(clusters['DEC'])\n",
    "  cl_z= np.array(clusters['Z'])\n",
    "  cl = np.array([cl_RA,cl_DEC,cl_z]).T\n",
    "  clz = zip(cl_RA,cl_DEC,cl_z)\n",
    "  clzlist = [x for x in clz]\n",
    "\n",
    "  results = pool.map(survey_lensing, clzlist)\n",
    "\n",
    "  stick = [clusters]\n",
    "  \n",
    "  \n",
    "  radii = np.logspace(-0.8,0.8,8)\n",
    "  N = len(radii)\n",
    "  bins_lims = np.logspace(np.log10(radii[0])+(np.log10(radii[0])-np.log10(radii[1]))/2,\n",
    "                          np.log10(radii[N-1])-(np.log10(radii[0])-np.log10(radii[1]))/2,N+1)\n",
    "  bins_lims = np.array([[bins_lims[i],bins_lims[i+1]] for i in range(N)])\n",
    "\n",
    "\n",
    "  Nboot=200\n",
    "  stick_results = []\n",
    "  for stake in stick:\n",
    "      t = time.time()\n",
    "      clusterbkgs = []\n",
    "      for index in stake['INDEX']:\n",
    "          Sigma_crit = np.array(results[index]['Critical Density'])\n",
    "          e_t = np.array(results[index]['Tangential Shear'])\n",
    "          e_x = np.array(results[index]['Cross Shear'])\n",
    "          W = np.array(results[index]['Weights'])\n",
    "          M = np.array(results[index]['Mult. Bias'])\n",
    "          R = np.array(results[index]['Radial Distance'])\n",
    "          clusterbkgs.append(np.array([Sigma_crit, e_t, e_x, W, R,M]))\n",
    "      print(len(clusterbkgs))\n",
    "      sigmas, sigmas_cov, xigmas, xigmas_cov = stack(clusterbkgs,bins_lims,Nboot)\n",
    "      stick_results.append( ( sigmas, sigmas_cov, xigmas, xigmas_cov) )\n",
    "      print(\"Done in \" + str(time.time()-t) + \" seconds.\")\n",
    "     \n",
    "  def NFWsimple(theta,Z,radii):\n",
    "    logM200, C200  = theta\n",
    "    M200 = np.power(10,logM200)\n",
    "    result = xlensing.model.NFW_shear(M200, C200, Z, 1.0, 0.001, 1e10,radii)['NFW Signal'] #returns only the main shear signal - all other signals (incl cross signal) available see docstring\n",
    "    return result\n",
    "\n",
    "  M200lo, M200hi = 13, 15\n",
    "  C200lo, C200hi = 0, 10\n",
    "\n",
    "  priorM200 = xlensing.fitting.ln_flat_prior_maker(M200lo, M200hi,0)\n",
    "  priorC200 = xlensing.fitting.ln_flat_prior_maker(C200lo, C200hi,1)\n",
    "  #priorPCC = xlensing.fitting.ln_gaussian_prior_maker(0.75, 0.07,2) ##Zhang et al. 2019\n",
    "  prior = lambda theta : priorM200(theta) + priorC200(theta)# + priorPCC(theta)\n",
    "\n",
    "  ndim, nwalkers, steps = 2, 256, 256\n",
    "  samplestick = []\n",
    "  #for each stack, run MCMC\n",
    "  burnin=round(steps/4.)\n",
    "  for stickresult in stick_results:\n",
    "    mean_z = Zcluster\n",
    "\n",
    "    #build data likelihood\n",
    "    model = lambda theta: NFWsimple(theta,mean_z,radii)\n",
    "    likelihood = xlensing.fitting.ln_gaussian_likelihood_maker((stickresult[0],stickresult[1]),model)\n",
    "    posterior = lambda theta : likelihood(theta) +prior(theta)\n",
    "\n",
    "    #initialise walkers\n",
    "    pos = []\n",
    "    for i in range(nwalkers):\n",
    "        M200 = np.random.uniform(M200lo,M200hi)\n",
    "        C200 = np.random.uniform(C200lo,C200hi)\n",
    "        #PCC  = np.random.uniform(PCClo,PCChi)\n",
    "        pos.append(np.array([M200,C200]))\n",
    "\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, posterior)\n",
    "    print(\"Running MCMC...\")\n",
    "    t = time.time()\n",
    "    sampler.run_mcmc(pos, steps, rstate0=np.random.get_state())\n",
    "    print(\"Done in \" + str(time.time()-t) + \" seconds.\")\n",
    "    samples = sampler.chain[:, burnin:, :].reshape((-1, ndim))\n",
    "    samplestick.append(samples)\n",
    "  for samples in samplestick:\n",
    "    mvir_tru,conc_tru= map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), \n",
    "                           zip(*np.percentile(samples, [16, 50, 84],axis=0)))\n",
    "    #print(\"Mvir: {:.2e}\".format(mvir_tru[0]) + \" p {:.2e}\".format(mvir_tru[1]) + \" m {:.2e}\".format(mvir_tru[2]))\n",
    "    #print(\"Conc: {:.2f}\".format(conc_tru[0]) + \" p {:.2f}\".format(conc_tru[1]) + \" m {:.2f}\".format(conc_tru[2]))\n",
    "  \n",
    "  m_ratio = 10**mvir_tru[0]/M200true\n",
    "  c_ratio = conc_tru[0]/C200true\n",
    "  print(m_ratio)\n",
    "  print(c_ratio)\n",
    "  mratio_list.append(m_ratio)\n",
    "  cratio_list.append(c_ratio)\n",
    "  np.save(\"mratio_lower_snr.npy\",np.array(mratio_list))\n",
    "  np.save(\"cratio_lower_snr.npy\",np.array(cratio_list))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_lambda_MEP(Lambda):\n",
    "    Lambda0 = 40\n",
    "    M0 = 2.21E14\n",
    "    alpha = 1.18\n",
    "    \n",
    "    mass = M0*(Lambda/Lambda0)**alpha\n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_DuttonMaccio(z, m):\n",
    "    \"\"\"Concentration from c(M) relation in Dutton & Maccio (2014).\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : float or array_like\n",
    "        Redshift(s) of halos.\n",
    "    m : float or array_like\n",
    "        Mass(es) of halos (m200 definition), in units of solar masses.\n",
    "    h : float, optional\n",
    "        Hubble parameter. Default is from Planck13.\n",
    "    Returns\n",
    "    ----------\n",
    "    ndarray\n",
    "        Concentration values (c200) for halos.\n",
    "    References\n",
    "    ----------\n",
    "    Calculation from Planck-based results of simulations presented in:\n",
    "    A.A. Dutton & A.V. Maccio, \"Cold dark matter haloes in the Planck era:\n",
    "    evolution of structural parameters for Einasto and NFW profiles,\"\n",
    "    Monthly Notices of the Royal Astronomical Society, Volume 441, Issue 4,\n",
    "    p.3359-3374, 2014.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    a = 0.52 + 0.385 * np.exp(-0.617 * (z**1.21))  # EQ 10\n",
    "    b = -0.101 + 0.026 * z                         # EQ 11\n",
    "\n",
    "    logc200 = a + b * np.log10(m * 1 / (10.**12))  # EQ 7\n",
    "\n",
    "    concentration = 10.**logc200\n",
    "\n",
    "    return concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_DuttonMaccio(.3,4e14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_z = [0.3, 0.2, 0.35, 0.4, 0.5]\n",
    "clusters_M = [2, 1, 3, 4, 2 ] #1e14\n",
    "clusters_C = [c_DuttonMaccio(z, m*1e14)+np.random.normal(0,c_DuttonMaccio(z, m*1e14)/20) \n",
    "              for z,m in zip(clusters_z,clusters_M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngals = 1e5\n",
    "\n",
    "E1gals, E2gals, RAgals, DECgals, Zgals = gen_gal(Ngals=Ngals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M200lo, M200hi = 13, 15\n",
    "C200lo, C200hi = 0, 10\n",
    "priorM200 = xlensing.fitting.ln_flat_prior_maker(M200lo, M200hi,0)\n",
    "priorC200 = xlensing.fitting.ln_flat_prior_maker(C200lo, C200hi,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda theta: NFWsimple(theta,mean_z,radii)\n",
    "likelihood = xlensing.fitting.ln_gaussian_likelihood_maker((stickresult[0],stickresult[1]),model)\n",
    "posterior = lambda theta : likelihood(theta) +prior(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
